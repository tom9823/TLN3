{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione 2\n",
    "\n",
    "Sempre partendo dai dati sulle definizioni, si richiede di provare a costruire un sistema che utilizzi la molteplicità delle definizioni per risalire al termine \"target\" in maniera automatica. Non si richiede di \"indovinare\" ogni termine, ma di avvicinarsi (almeno semanticamente) alla risposta. Provare più soluzioni, includendo meccanismi di filtro delle definizioni (ad es. escludendo quelle meno informative o con caratteristiche particolari), di ricerca nell'albero tassonomico di WordNet (provando a partire da candidati \"genus\", secondo il principio Genus-Differentia), ecc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodi di supporto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizzazione della frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    tokens = [token for token in tokens if token not in string.punctuation] #tolgo la punteggiatura\n",
    "    tokens = [token.lower() for token in tokens] # sostituisco le maiuscole con le minuscole\n",
    "    tokens = [token for token in tokens if token not in stop_words] # rimuovo le stop words\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # lemmatizzo\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "def get_nouns(tokens):\n",
    "    #mantengo solo i sostantivi\n",
    "    tokens = nltk.pos_tag(tokens)\n",
    "    tokens = [word for word in tokens if word[1] in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    return [word[0] for word in tokens]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo Simplified Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(definitions):\n",
    "    context = set()\n",
    "    for definition in definitions:\n",
    "        context = context.union(set(definition))\n",
    "    return context\n",
    "\n",
    "def simplified_lesk(word, context):\n",
    "    best_sense = None\n",
    "    max_overlap = 0\n",
    "\n",
    "    for sense in wn.synsets(word, pos='n'):\n",
    "        signature = set(normalize(sense.definition())).union(set(normalize(' '.join(sense.examples()))))\n",
    "        overlap = len(context.intersection(signature))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "    \n",
    "    return best_sense\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ottenimento dei genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Restituisce come genus i token che compaiono più frequentemente nelle definizioni\n",
    "'''\n",
    "def get_genus(definitions, number_genus=3):\n",
    "    words = []\n",
    "    for definition in definitions:\n",
    "        words += get_nouns(definition)\n",
    "\n",
    "    return [tupla[0] for tupla in nltk.FreqDist(words).most_common(number_genus)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio 1: recursive simplified lesk\n",
    "\n",
    "Partendo dal genus, si prendono tutti i suoi synset, e ad ognuno di essi si applica il recursive_simplified_lesk. La caratteristica\n",
    "di questo algoritmo è che non si ferma al primo livello di iponimi del genus, ma per ognuno di essi scende di un certo numero di livelli (iperparametro da specificare). Così facendo vengono analizzati un numero maggiore di synsets così da avere più probabilità di ottenere il synset corretto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Variante ricorsiva del lesk, che permette di scendere di livello nella\n",
    "gerarchia del genus\n",
    "'''\n",
    "def recursive_simplified_lesk(level, senses, current_syn, context):\n",
    "    if level != 0:\n",
    "        for syn in current_syn.hyponyms():\n",
    "            branch_senses = []\n",
    "            signature = set(normalize(syn.definition())).union(normalize(' ' .join(syn.examples())))\n",
    "            overlap = len(context.intersection(signature))\n",
    "            branch_senses.append((syn,overlap))\n",
    "            recursive_simplified_lesk(level - 1, branch_senses, syn, context)\n",
    "            senses = senses + branch_senses\n",
    "        return senses\n",
    "\n",
    "'''\n",
    "Metodo per predire il token dato un insieme di definizioni\n",
    "'''\n",
    "def predict_token(definitions):\n",
    "    ret = {}\n",
    "    genus = get_genus(definitions)\n",
    "    # ottengo il contesto\n",
    "    context = get_context(definitions)\n",
    "\n",
    "    for gen in genus:\n",
    "        for syn_genus in wn.synsets(gen, pos='n')[0:3]:\n",
    "            senses = []\n",
    "            ret[syn_genus] = recursive_simplified_lesk(3, senses, syn_genus, context)\n",
    "\n",
    "    array_concatenato = []\n",
    "    for array in ret.values():\n",
    "        array_concatenato.extend(array)\n",
    "    return sorted(array_concatenato, key=lambda tupla: tupla[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: door\n",
      "[(Synset('opening.n.01'), 6), (Synset('entrance.n.01'), 5), (Synset('compartment.n.02'), 4), (Synset('study.n.05'), 4), (Synset('headroom.n.01'), 4)]\n",
      "\n",
      "\n",
      "token: ladybug\n",
      "[(Synset('dipterous_insect.n.01'), 4), (Synset('leaf_miner.n.01'), 3), (Synset('lepidopterous_insect.n.01'), 3), (Synset('psocopterous_insect.n.01'), 3), (Synset('web_spinner.n.01'), 3)]\n",
      "\n",
      "\n",
      "token: pain\n",
      "[(Synset('taste.n.01'), 3), (Synset('vision.n.03'), 3), (Synset('masking.n.02'), 2), (Synset('smell.n.01'), 2), (Synset('sound.n.02'), 2)]\n",
      "\n",
      "\n",
      "token: blurriness\n",
      "[(Synset('collage.n.01'), 6), (Synset('naked_eye.n.01'), 4), (Synset('mental_picture.n.01'), 3), (Synset('visual_image.n.01'), 3), (Synset('iconography.n.01'), 3)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus1 = pd.read_csv('definizioni.tsv', sep='\\t', engine='python')\n",
    "# remove sentence too long or too short\n",
    "# Calcolo dei percentili per determinare i limiti\n",
    "q5 = corpus1.apply(lambda colonna: colonna.apply(len)).quantile(0.05)\n",
    "q95 = corpus1.apply(lambda colonna: colonna.apply(len)).quantile(0.95)\n",
    "# Sostituzione delle stringhe con la stringa vuota\n",
    "corpus1 = corpus1.apply(lambda colonna: colonna if colonna.dtype != object else np.where((colonna.str.len() < q5[colonna.name]) | (colonna.str.len() > q95[colonna.name]), \"\", colonna))\n",
    "\n",
    "corpus1['door'] = corpus1['door'].apply(normalize)\n",
    "corpus1['ladybug'] = corpus1['ladybug'].apply(normalize)\n",
    "corpus1['pain'] = corpus1['pain'].apply(normalize)\n",
    "corpus1['blurriness'] = corpus1['blurriness'].apply(normalize)\n",
    "\n",
    "for token in corpus1.columns:\n",
    "    print('token:', token)\n",
    "    predicted_token = predict_token(corpus1[token])\n",
    "    print(predicted_token)\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
